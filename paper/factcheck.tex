\documentclass{chi2009}
\usepackage{times}
\usepackage{url}
\usepackage{graphics}
\usepackage{color}
\usepackage{listings}
\usepackage[pdftex]{hyperref}
\hypersetup{%
pdftitle={Your Title},
pdfauthor={Your Authors},
pdfkeywords={your keywords},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}
\newcommand{\comment}[1]{}
\definecolor{Orange}{rgb}{1,0.5,0}
\newcommand{\todo}[1]{\textsf{\textbf{\textcolor{Orange}{[[#1]]}}}}

\pagenumbering{arabic}  % Arabic page numbers for submission.  Remove this line to eliminate page numbers for the camera ready copy

\begin{document}
% to make various LaTeX processors do the right thing with page size
\special{papersize=8.5in,11in}
\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

% use this command to override the default ACM copyright statement 
% (e.g. for preprints). Remove for camera ready copy.
\toappear{Submitted for review to CHI 2009.}

\title{Natural Language Fact Checker}
\numberofauthors{2}
\author{
  \alignauthor Joshua Kraunelis\\
    \affaddr{University of Massachusetts Lowell}\\
    \affaddr{1 University Ave, Lowell, MA 01852}\\
    \email{jkraunel@cs.uml.edu}
  \alignauthor Wesley Nuzzo\\
    \affaddr{University of Massachusetts Lowell}\\
    \affaddr{1 University Ave, Lowell, MA 01852}\\
    \email{uraniumcronorum@gmail.com}
}

\maketitle

\begin{abstract}
  In this paper we describe the formatting requirements for SIGCHI
  Conference Proceedings, and offer recommendations on writing for the
  worldwide SIGCHI readership.  Please review this document even if
  you have submitted to SIGCHI conferences before, for some format
  details have changed relative to previous years. These include the
  formatting of table captions, the formatting of references, and a
  requirement to include ACM DL indexing information.
\end{abstract}

\keywords{automated fact checking} 

\section{Introduction}

Fact checking is the task of finding evidence to validate or invalidate a given claim. This discipline is most widely applied by journalistic organizations such as newspapers, magazines, and radio/television broadcasters.  However, fact checking isn't limited to journalism, it is also necessary in other venues such as academic publishing.  Fact checking is either applied either before (ante hoc) or after (post hoc) information has been released, and may be performed by an individual within an organization (e.g. journalist) or by an outside fact checking organization like factcheck.org \cite{factcheck} or Politifact \cite{politifact}.  In any case, whether the fact checking is done for print or broadcast media, by internal or external agents, or for a news article or journal publication, the fact checking is always performed by a human being.  With more people getting their news from social media outlets such as Facebook and Twitter, there is a need to scale up fact checking to handle the tremendous volume and rate at which stories are disseminated.  In this paper, we propose a system which can verify basic types of claims automatically, without the need for a human fact checker.

\subsection{Related Work}
Researchers at Fullfact.org \cite{babakar_moy_2016} define three main approaches to automated fact checking: reference, machine learning, and context.  The reference approach relies on some official source of information to validate the claim.  The machine learning approach uses some model of how the world works to estimate the likelihood of a claim.  The contextual approach looks at the prevalence of a claim to decide how likely it might be.

Hassan et al. \cite{hassan2015quest} present a system for identifying claims automatically via machine learning.  Their software, ClaimBuster, performs natural language processing tasks such as sentiment analysis, tokenization, part of speech tagging, and entity resolution to classify a sentence as non-factual, unimportant factual, or check-worthy factual.  This system reduces the amount of time spent searching for claims to be validated, though it does not perform the validation itself.  

Ciampaglia et al. \cite{ciampaglia2015computational} take a graph-theoretic approach to automated fact checking.  They encode knowledge from Wikipedia into graph data structures called knowledge graphs that, together, form larger structures called knowledge networks.  They show that traditional graph/network analysis techniques such as shortest path computation can be used to efficiently evaluate the truthfulness of a claim.


\section{Project Description}

The Natural Language Fact Checker employs concepts from two major areas of artificial intelligence: Natural Language Processing and Logic Programming.  The natural language processing component is necessary for the identification and semantic interpretation of individual words or phrases contained within an English sentence (i.e. a claim, in the context of the fact checker).  The semantics of a claim are used to extract zero or more assertions about the claim.  Those assertions are supplied to the logic component.  

\subsection{Natural Language Processing}

This section describes the key techniques, technologies and tools used in the natural language processing component.  

All of the NLP code was implemented using the Python Natural Language Toolkit (NLTK) \cite{nltk}.  We chose to work with NLTK for many reasons, including our familiarity with the Python programming language, the availability of a number of NLP methods, the integration of numerous well known corpora, and the extensive collection of documentation and tutorials available online.
 
For the fact checker, our NLP pipeline consists of the following four steps:

\begin{itemize}
   \item Tokenization
   \item Part-of-speech tagging
   \item Named entity tagging
   \item Relation Extraction
\end{itemize}

Tokenization is the process by which a sentence is segmented into smaller pieces (i.e. words, punctuation, etc.).  Many NLP applications employ sentence segmentation to break a larger body of text into separate sentences, then apply tokenization to each sentence.  Our fact checker is designed to process only a single sentence at a time, therefore we have no need for sentence segmentation.  To perform tokenization, we use NLTK's word\_tokenize API, which takes in a single string sentence and produces a list of strings representing the tokens in the sentence.  The word\_tokenize method calls two underlying NLTK tokenization algorithms, TreeBankWordTokenizer and PunktSentenceTokenizer, which perform tokenization using a combination of predetermined regular expressions and a pretrained unsupervised model. Listing \ref{tokList} shows an example of tokenization using NLTK.

\begin{lstlisting}[caption=NLTK Tokenization Example, label=tokList]
>>> from nltk import word_tokenize
>>> word_tokenize(``This is a sentence that has been tokenized using NLTK's word_tokenize API.'')
['This', 'is', 'a', 'sentence', 'that', 'has', 'been', 'tokenized', 'using', 'NLTK', ``'s'', 'word_tokenize', 'API', '.']
>>> 
\end{lstlisting}



\subsection{Logic Programming}

\subsection{Scraper}

\subsection{Web UI}


\section{Analysis of Results}


\subsection{Discussion}

Your paper's title, authors and affiliations should run across the
full width of the page in a single column 17.8 cm (7 in.) wide.  The
title should be in Helvetica 18-point bold; use Arial if Helvetica is
not available.  Authors' names should be in Times Roman 12-point bold,
and affiliations in Times Roman 12-point (note that Author and
Affiliation are defined Styles in this template file).

To position names and addresses, use a single-row table with invisible
borders, as in this document.  Alternatively, if only one address is
needed, use a centered tab stop to center all name and address text on
the page; for two addresses, use two centered tab stops, and so
on. For more than three authors, you may have to place some address
information in a footnote, or in a named section at the end of your
paper. Please use full international addresses and telephone dialing
prefixes.  Leave one 10-pt line of white space below the last line of
affiliations.

\section{Conclusions}

Every submission should begin with an abstract of about 150 words,
followed by a set of keywords. The abstract and keywords should be
placed in the left column of the first page under the left half of the
title. The abstract should be a concise statement of the problem,
approach and conclusions of the work described.  It should clearly
state the paper's contribution to the field of HCI.

The first set of keywords will be used to index the paper in the
proceedings. The second set are used to catalogue the paper in the ACM
Digital Library. The latter are entries from the ACM Classification
System~\cite{acm_categories}.  In general, it should only be necessary
to pick one or more of the H5 subcategories, see
http://www.acm.org/class/1998/H.5.html

\section{Acknowledgments}

Please use a 10-point Times Roman font or, if this is unavailable,
another proportional font with serifs, as close as possible in
appearance to Times Roman 10-point. The Press 10-point font available
to users of Script is a good substitute for Times Roman. If Times
Roman is not available, try the font named Computer Modern Roman. On a
Macintosh, use the font named Times and not Times New Roman. Please
use sans-serif or non-proportional fonts only for special purposes,
such as headings or source code text.

\bibliographystyle{abbrv}
\bibliography{factcheck}

\end{document}
