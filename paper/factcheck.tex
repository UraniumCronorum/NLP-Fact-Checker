\documentclass{chi2009}
\usepackage{times}
\usepackage{url}
\usepackage{graphics}
\usepackage{color}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{hyperref}
\hypersetup{%
pdftitle={Your Title},
pdfauthor={Your Authors},
pdfkeywords={your keywords},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}
\newcommand{\comment}[1]{}
\definecolor{Orange}{rgb}{1,0.5,0}
\newcommand{\todo}[1]{\textsf{\textbf{\textcolor{Orange}{[[#1]]}}}}

\pagenumbering{arabic}  % Arabic page numbers for submission.  Remove this line to eliminate page numbers for the camera ready copy

\begin{document}
% to make various LaTeX processors do the right thing with page size
\special{papersize=8.5in,11in}
\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

% use this command to override the default ACM copyright statement 
% (e.g. for preprints). Remove for camera ready copy.
%\toappear{Submitted for review to CHI 2009.}

\title{Natural Language Fact Checker}
\numberofauthors{2}
\author{
  \alignauthor Joshua Kraunelis\\
    \affaddr{University of Massachusetts Lowell}\\
    \affaddr{1 University Ave, Lowell, MA 01852}\\
    \email{jkraunel@cs.uml.edu}
  \alignauthor Wesley Nuzzo\\
    \affaddr{University of Massachusetts Lowell}\\
    \affaddr{1 University Ave, Lowell, MA 01852}\\
    \email{uraniumcronorum@gmail.com}
}

\maketitle

\begin{abstract}
  In this paper we describe the formatting requirements for SIGCHI
  Conference Proceedings, and offer recommendations on writing for the
  worldwide SIGCHI readership.  Please review this document even if
  you have submitted to SIGCHI conferences before, for some format
  details have changed relative to previous years. These include the
  formatting of table captions, the formatting of references, and a
  requirement to include ACM DL indexing information.
\end{abstract}

\keywords{automated fact checking} 

\section{Introduction}

Fact checking is the task of finding evidence to validate or invalidate a given claim. This discipline is most widely applied by journalistic organizations such as newspapers, magazines, and radio/television broadcasters.  However, fact checking isn't limited to journalism, it is also necessary in other venues such as academic publishing.  Fact checking is either applied either before (ante hoc) or after (post hoc) information has been released, and may be performed by an individual within an organization (e.g. journalist) or by an outside fact checking organization like factcheck.org \cite{factcheck} or Politifact \cite{politifact}.  In any case, whether the fact checking is done for print or broadcast media, by internal or external agents, or for a news article or journal publication, the fact checking is always performed by a human being.  With more people getting their news from social media outlets such as Facebook and Twitter, there is a need to scale up fact checking to handle the tremendous volume and rate at which stories are disseminated.  In this paper, we propose a system which can verify basic types of claims automatically, without the need for a human fact checker.

\subsection{Related Work}
Researchers at Fullfact.org \cite{babakar_moy_2016} define three main approaches to automated fact checking: reference, machine learning, and context.  The reference approach relies on some official source of information to validate the claim.  The machine learning approach uses some model of how the world works to estimate the likelihood of a claim.  The contextual approach looks at the prevalence of a claim to decide how likely it might be.

Hassan et al. \cite{hassan2015quest} present a system for identifying claims automatically via machine learning.  Their software, ClaimBuster, performs natural language processing tasks such as sentiment analysis, tokenization, part of speech tagging, and entity resolution to classify a sentence as non-factual, unimportant factual, or check-worthy factual.  This system reduces the amount of time spent searching for claims to be validated, though it does not perform the validation itself.  

Ciampaglia et al. \cite{ciampaglia2015computational} take a graph-theoretic approach to automated fact checking.  They encode knowledge from Wikipedia into graph data structures called knowledge graphs that, together, form larger structures called knowledge networks.  They show that traditional graph/network analysis techniques such as shortest path computation can be used to efficiently evaluate the truthfulness of a claim.


\section{Project Description}

The Natural Language Fact Checker employs concepts from two major areas of artificial intelligence: Natural Language Processing and Logic Programming.  The natural language processing component is necessary for the identification and semantic interpretation of individual words or phrases contained within an English sentence (i.e. a claim, in the context of the fact checker).  The semantics of a claim are used to extract zero or more assertions about the claim.  Those assertions are supplied to the logic component.  

\subsection{Natural Language Processing}

This section describes the key techniques, technologies and tools used in the natural language processing component.  

All of the NLP code was implemented using the Python Natural Language Toolkit (NLTK) \cite{nltk}.  We chose to work with NLTK for many reasons, including our familiarity with the Python programming language, the availability of a number of NLP methods, the integration of numerous well known corpora, and the extensive collection of documentation and tutorials available online.
 
For the fact checker, our NLP pipeline consists of the following four steps:

\begin{itemize}
   \item Tokenization
   \item Part-of-speech tagging
   \item Named-entity Extraction
   \item Relation Extraction
\end{itemize}

Tokenization is the process by which a sentence is segmented into smaller pieces (i.e. words, punctuation, etc.).  Many NLP applications employ sentence segmentation to break a larger body of text into separate sentences, then apply tokenization to each sentence.  Our fact checker is designed to process only a single sentence at a time, therefore we have no need for sentence segmentation.  To perform tokenization, we use NLTK's word\_tokenize API, which takes in a single string sentence and produces a list of strings representing the tokens in the sentence.  The word\_tokenize method calls two underlying NLTK tokenization algorithms, TreeBankWordTokenizer and PunktSentenceTokenizer, which perform tokenization using a combination of predetermined regular expressions and a pretrained unsupervised model. Listing \ref{tokList} shows an example of tokenization using NLTK.

\begin{lstlisting}[caption=NLTK Tokenization Example, label=tokList]
>>> from nltk import word_tokenize
>>> word_tokenize(``This is a sentence
 that has been tokenized using NLTK's 
word_tokenize API.'')
['This', 'is', 'a', 'sentence', 'that',
 'has', 'been', 'tokenized', 'using', 
'NLTK', ``'s'', 'word_tokenize', 
'API', '.']
\end{lstlisting}

Part-of-speech tagging is the process of identifying the part-of-speech for a given token.  The NLTK pos\_tag method relies on a pretrained perceptron model to classify a token's part-of-speech based on its context.  The pos\_tag method takes a list of string tokens and produces a list of token-tag pairs.  Each tag is represented as an abbreviation of the part-of-speech, and may include information about plurality, possession, and tense.  A collection of tags is referred to as a tagset.  Listing 2 shows an example of the NLTK pos\_tag capability.

\begin{lstlisting}[caption=NLTK POS Tagger Example, label=tagList]
>>> from nltk import pos_tag
>>> pos_tag(tokens)
[('This', 'DT'), ('sentence', 'NN'),
 ('has', 'VBZ'), ('been', 'VBN'), 
('tagged', 'VBN'), ('using', 'VBG'),
 ('NLTK', 'NNP'), (``'s'', 'POS'), 
('pos_tag', 'NN'), ('API', 'NNP'), 
('.', '.')]
\end{lstlisting}

By default, NLTK uses the UPenn tagset \cite{upenn_tags}.  Table \ref{tableTags} describes the tags shown in the example.

\begin{table}
\begin{center}
\begin{tabular}{ | l l | }
\hline
Tag & Definition \\
\hline
DT & determiner \\
NN & singular noun \\ 
NNP & proper noun \\ 
POS & possessive marker \\
VBG & verb, gerund \\ 
VBN & verb, past participle \\
VBZ & verb, 3rd person singular \\
. & period \\
\hline
\end{tabular}
\caption{Some UPenn Tagset Definitions}
\label{tableTags}
\end{center}
\end{table}

Named-entity extraction is the process of identifying tokens or phrases which refer to named entities within text.  Typical named-entities include people, locations, cardinal numbers, and organizations.  NLTK has several named-entity recognizer APIs, however, we chose to use the NLTK interface to the Stanford Named-Entity Recognizer \cite{stanfordner}.  The Stanford NER is a standalone Java tool that must be installed separately, but may be called from Python code using the NLTK interface.  The Stanford NER uses one or more conditional random field models trained on the CoNLL 2003 eng \cite{conll2003} and MUC 6/7 \cite{muc6} datasets to identify named entities.  The API for the named-entity extractor is similar to the POS tagger - it expects a list of tokens and returns a list of token, tag pairs, where the tags are one of 4 classes: location, person, organization, or other.

Relation extraction is the process of determining a relationship between one or more entities.  NLTK provides APIs for relation extraction, however we were not able to get them working to our satisfaction.  Instead of using the provided APIs for relation extraction, we took an approach using regular expressions.  Using the Python re module, We created a series of regular expressions that would match if text indicating a certain relationship was present.

By leveraging NLTK and Stanford's named-entity recognizer, we were able to avoid the time consumings tasks of data annotation and model training.

\subsection{Logic Programming}


\subsection{Data}
Due to time constraints, we restricted our fact checker to the domain of American professional basketball.  The reasons for this were many:

\begin{itemize}
\item The domain is well known and recognizable by most people (i.e. even non-basketball fans are likely to recognize an athlete such as LeBron James).
\item The roster information and player physical characteristics do not change frequently, therefore the knowledge we encode is likely to be valid for a long duration (i.e. player height is not likely to change over time).
\item The knowledge we chose to represent is well defined and publicly available from many sources.
\item The number of athletes is static and low enough (30 teams in NBA, 15 players per team) that we were able to represent the complete set of all current NBA players without much effort.
\end{itemize}

In order to encode this knowledge into the logic base, we wrote a Python script that would scrape the roster data from https://en.wikipedia.org/wiki/List\_of\_current\_NBA\_team\_rosters.  The script leveraged BeautifulSoup4 \cite{beautifulsoup}, a Python package that provides an easy to use parser for structured documents such as HTML.  The Wikipedia page was scraped for player team affiliation, player height, player weight, and player age.  The height was represented in meters, weight in pounds, and age as number of years old.  Each of these characteristics was encoded as a Pyke rule, as shown in Table \ref{tableRules}.  We applied normalization to the player and team names such that all characters were converted to lowercase, special characters (e.g. spaces, hyphens, apostrophes) were replaced with the underscore character, and non-English alphabet characters were replaced with their corresponding English alphabet character (or as close as we could get).  For example, the character é would be replaced by the character e.

  

\begin{table}
\begin{center}
\begin{tabular}{ | l | }
\hline
Rule \\
\hline 
plays\_for(seth\_curry,dallas\_mavericks) \\
height(mario\_hezonja,2.03) \\
height(jerami\_grant,2.03) \\
weight(thon\_maker,223) \\
age(marshall\_plumlee,24) \\
age(kyrie\_irving,24) \\
plays\_for(rodney\_stuckey,indiana\_pacers) \\
plays\_for(hollis\_thompson,philadelphia\_76ers) \\
age(andre\_iguodala,32) \\
height(brandon\_knight,1.91) \\
\hline
\end{tabular}
\caption{Random Sample of Pyke Rules}
\label{tableRules}
\end{center}
\end{table}

\subsection{Web UI}


\section{Analysis of Results}


\section{Discussion}

Your paper's title, authors and affiliations should run across the
full width of the page in a single column 17.8 cm (7 in.) wide.  The
title should be in Helvetica 18-point bold; use Arial if Helvetica is
not available.  Authors' names should be in Times Roman 12-point bold,
and affiliations in Times Roman 12-point (note that Author and
Affiliation are defined Styles in this template file).

To position names and addresses, use a single-row table with invisible
borders, as in this document.  Alternatively, if only one address is
needed, use a centered tab stop to center all name and address text on
the page; for two addresses, use two centered tab stops, and so
on. For more than three authors, you may have to place some address
information in a footnote, or in a named section at the end of your
paper. Please use full international addresses and telephone dialing
prefixes.  Leave one 10-pt line of white space below the last line of
affiliations.

\subsection{Ethical implications of this work}

Generally, the purpose of fact-checking of this sort is to help people make decisions between opposing viewpoints, based on whether the facts their arguments depend on are accurate.
It can also provide a proxy for judging a politician's trustworthiness, as a politician who relies on a lot of falsehoods likely cannot be trusted as much as one who does not.

This means that users could rely on technology like this to make very important judgments; often judgements that would impact the result of elections.
Because it is important for these judgements to be sound, designers of a technology like this have a clear responsibility to make the meaning of the results clear, and to make the process as transparent as possible.

Ideally, in order to allow users to make their own judgments about the meaning of the results, the AI should not just output either True or False (or some intermediate value), but also should show the basis for those results, including sources.
Our implementation does this to some extent--see the debug mode in the web UI, and note that our knowledge base is capable of keeping track of the sources for its claims.

Another concern for an implementation of an AI like this is how the knowledge base is populated.
For example, if some of the information comes from social media websites or other sources where personal information might be available, this raises the issue of how to handle users' privacy.

Additionally, some potential sources, such as WikiLeaks, have questionable legal and ethical status.
The decision about whether or not to use a source like this could depend on an ethical judgment about whether it is okay to use information that has been distributed in violation of laws--or indeed whether there is an obligation to distribute censored information as an act of civil disobedience.

\section{Conclusions}

Every submission should begin with an abstract of about 150 words,
followed by a set of keywords. The abstract and keywords should be
placed in the left column of the first page under the left half of the
title. The abstract should be a concise statement of the problem,
approach and conclusions of the work described.  It should clearly
state the paper's contribution to the field of HCI.

The first set of keywords will be used to index the paper in the
proceedings. The second set are used to catalogue the paper in the ACM
Digital Library. The latter are entries from the ACM Classification
System~\cite{acm_categories}.  In general, it should only be necessary
to pick one or more of the H5 subcategories, see
http://www.acm.org/class/1998/H.5.html

\section{Acknowledgments}

Please use a 10-point Times Roman font or, if this is unavailable,
another proportional font with serifs, as close as possible in
appearance to Times Roman 10-point. The Press 10-point font available
to users of Script is a good substitute for Times Roman. If Times
Roman is not available, try the font named Computer Modern Roman. On a
Macintosh, use the font named Times and not Times New Roman. Please
use sans-serif or non-proportional fonts only for special purposes,
such as headings or source code text.

\bibliographystyle{abbrv}
\bibliography{factcheck}

\end{document}
